{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUlY/kzCO8f/gmmTEmNxpL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritikpathania/project/blob/main/sentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raN8ARJ6XTmg",
        "outputId": "33b3801b-056f-4ed6-d236-7e9a0bf96a77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1AAdD7d1TZSQE8mYFZI8_febLIwgznxhR\n",
            "To: /content/dataset.zip\n",
            "100% 20.9M/20.9M [00:00<00:00, 49.5MB/s]\n",
            "Extracted files: ['__MACOSX', 'Twitter_Data.csv']\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Define the local path where you want to save the dataset zip file\n",
        "local_zip_path = '/content/dataset.zip'\n",
        "\n",
        "# Download the zip file from the Google Drive link\n",
        "!gdown --id 1AAdD7d1TZSQE8mYFZI8_febLIwgznxhR -O \"$local_zip_path\"\n",
        "\n",
        "# Specify the local directory where you want to extract the files\n",
        "extraction_path = '/content/dataset'\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs(extraction_path, exist_ok=True)\n",
        "\n",
        "# Unzip the dataset\n",
        "import zipfile\n",
        "with zipfile.ZipFile('/content/gdrive/MyDrive/archive.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(extraction_path)\n",
        "\n",
        "# List the contents of the extraction directory to verify\n",
        "extracted_files = os.listdir(extraction_path)\n",
        "print(f\"Extracted files: {extracted_files}\")\n",
        "\n",
        "# Load the dataset from the extracted directory\n",
        "dataset_dir = '/content/dataset'\n",
        "dataset_path = f\"/content/dataset/Twitter_Data.csv\"  # Replace with the actual CSV file\n",
        "dataset = pd.read_csv(dataset_path)\n",
        "\n",
        "# Split the dataset into training and validation data\n",
        "train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out non-empty and non-null text values\n",
        "train_data = train_data.dropna(subset=['clean_text'])\n",
        "train_data = train_data[train_data['clean_text'] != '']\n",
        "\n",
        "# Map your labels to 0, 1, and 2\n",
        "train_data['category'] = train_data['category'].map({-1: 0, 0: 1, 1: 2})\n",
        "val_data['category'] = val_data['category'].map({-1: 0, 0: 1, 1: 2})"
      ],
      "metadata": {
        "id": "eJJ6G51PjT4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text using BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "X_train = tokenizer(list(train_data['clean_text']), padding='max_length', truncation=True, max_length=128, return_tensors='tf', return_token_type_ids=False)\n",
        "X_val = tokenizer(list(val_data['clean_text']), padding='max_length', truncation=True, max_length=128, return_tensors='tf', return_token_type_ids=False)\n",
        "\n",
        "# Convert to TensorFlow tensors\n",
        "X_train = {key: tf.convert_to_tensor(X_train[key]) for key in X_train}\n",
        "X_val = {key: tf.convert_to_tensor(X_val[key]) for key in X_val}\n",
        "\n",
        "# Prepare labels\n",
        "y_train = train_data['category'].to_numpy()\n",
        "y_val = val_data['category'].to_numpy()\n",
        "\n",
        "# Build the BERT-based model\n",
        "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)  # Three classes\n",
        "\n",
        "# Compile the model with the appropriate loss function\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cO1jRp1NjXYA",
        "outputId": "6d157ea7-76e0-4148-bb31-cf8300a9c8d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=3, batch_size=8)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_val, y_val)\n",
        "print(f'Validation Loss: {loss:.4f}, Validation Accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29gOMv1ljMsQ",
        "outputId": "671429ef-6a5b-41f8-ded8-2b324d8cd2f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "100/100 [==============================] - 1688s 16s/step - loss: 1.0235 - accuracy: 0.4987 - val_loss: 1.0071 - val_accuracy: 0.5000\n",
            "Epoch 2/3\n",
            "100/100 [==============================] - 1635s 16s/step - loss: 0.9387 - accuracy: 0.5589 - val_loss: 0.9768 - val_accuracy: 0.5400\n",
            "Epoch 3/3\n",
            "100/100 [==============================] - 1623s 16s/step - loss: 0.7531 - accuracy: 0.6742 - val_loss: 0.9380 - val_accuracy: 0.5450\n",
            "7/7 [==============================] - 111s 15s/step - loss: 0.9380 - accuracy: 0.5450\n",
            "Validation Loss: 0.9380, Validation Accuracy: 0.5450\n"
          ]
        }
      ]
    }
  ]
}